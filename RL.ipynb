{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.4"
    },
    "colab": {
      "name": "RL_muZero.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPydH9ic9RH-"
      },
      "source": [
        "# Reinforcement Learning Projet\r\n",
        "\r\n",
        "**Athors** : Yassine Filali, Mohamed Farhat and Mohamed El Khames Boumaiza"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mLzblyI9Puc"
      },
      "source": [
        "#Importing necessary libraries\r\n",
        "import numpy as np\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import time\r\n",
        "import copy\r\n",
        "import torch.optim as optim\r\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pW4JhU2I9rzi"
      },
      "source": [
        "Implementation of the class State"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGVp4juJFra3"
      },
      "source": [
        "class State:\r\n",
        "  def __init__(self):\r\n",
        "        self.tictactoe = State_tictactoe \r\n",
        "        "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtFeqWPS9ucE"
      },
      "source": [
        "Implementation of a simple game: Tic-Tac-Toe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ppZnlVYGIdY"
      },
      "source": [
        "class State_tictactoe:\r\n",
        "    X, Y = 'ABC',  '123'\r\n",
        "    C = {0: '_', 1: 'O', -1: 'X'}\r\n",
        "\r\n",
        "    def __init__(self):\r\n",
        "        self.board = np.zeros((3, 3)) \r\n",
        "        self.color = 1\r\n",
        "        self.win_color = 0\r\n",
        "        self.record = []\r\n",
        "        self.BLACK = 1  # first turn\r\n",
        "        self.WHITE = -1 #second turn player\r\n",
        "\r\n",
        "    def action2str(self, a):\r\n",
        "        return self.X[a // 3] + self.Y[a % 3]\r\n",
        "\r\n",
        "    def str2action(self, s):\r\n",
        "        return self.X.find(s[0]) * 3 + self.Y.find(s[1])\r\n",
        "\r\n",
        "    def record_string(self):\r\n",
        "        return ' '.join([self.action2str(a) for a in self.record])\r\n",
        "\r\n",
        "    def __str__(self):\r\n",
        "        # output board.\r\n",
        "        s = '   ' + ' '.join(self.Y) + '\\n'\r\n",
        "        for i in range(3):\r\n",
        "            s += self.X[i] + ' ' + ' '.join([self.C[self.board[i, j]] for j in range(3)]) + '\\n'\r\n",
        "        s += 'record = ' + self.record_string()\r\n",
        "        return s\r\n",
        "\r\n",
        "    def play(self, action):\r\n",
        "        # state transition function\r\n",
        "        # action is position inerger (0~8) or string representation of action sequence\r\n",
        "        if isinstance(action, str):\r\n",
        "            for astr in action.split():\r\n",
        "                self.play(self.str2action(astr))\r\n",
        "            return self\r\n",
        "\r\n",
        "        x, y = action // 3, action % 3\r\n",
        "        self.board[x, y] = self.color\r\n",
        "\r\n",
        "        # check whether 3 stones are on the line\r\n",
        "        if self.board[x, :].sum() == 3 * self.color \\\r\n",
        "          or self.board[:, y].sum() == 3 * self.color \\\r\n",
        "          or (x == y and np.diag(self.board, k=0).sum() == 3 * self.color) \\\r\n",
        "          or (x == 2 - y and np.diag(self.board[::-1,:], k=0).sum() == 3 * self.color):\r\n",
        "            self.win_color = self.color\r\n",
        "\r\n",
        "        self.color = -self.color\r\n",
        "        self.record.append(action)\r\n",
        "        return self\r\n",
        "\r\n",
        "    def terminal(self):\r\n",
        "        # terminal state check\r\n",
        "        return self.win_color != 0 or len(self.record) == 3 * 3\r\n",
        "\r\n",
        "    def terminal_reward(self):\r\n",
        "        # terminal reward \r\n",
        "        return self.win_color if self.color == self.BLACK else -self.win_color\r\n",
        "\r\n",
        "    def legal_actions(self):\r\n",
        "        # list of legal actions on each state\r\n",
        "        return [a for a in range(3 * 3) if self.board[a // 3, a % 3] == 0]\r\n",
        "\r\n",
        "    def feature(self):\r\n",
        "        # input tensor for neural net (state)\r\n",
        "        return np.stack([self.board == self.color, self.board == -self.color]).astype(np.float32)\r\n",
        "\r\n",
        "    def action_feature(self, action):\r\n",
        "        # input tensor for neural net (action)\r\n",
        "        a = np.zeros((1, 3, 3), dtype=np.float32)\r\n",
        "        a[0, action // 3, action % 3] = 1\r\n",
        "        return a"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmACNHCN9zP_"
      },
      "source": [
        "An example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hrXXfHtGMBm",
        "outputId": "1655a580-cbab-481a-ec13-0373da2c5a03"
      },
      "source": [
        "state_ttt = State().tictactoe()\r\n",
        "print(\"Example 1 : \")\r\n",
        "action = 'A1 A2'\r\n",
        "state = state_ttt.play(action)\r\n",
        "print(state)\r\n",
        "print('Input feature after playing', action)\r\n",
        "print(state.feature())\r\n",
        "\r\n",
        "print(\"Example 2 : \")\r\n",
        "action = 'B1 A2 A1 B3'\r\n",
        "state = state_ttt.play(action)\r\n",
        "print(state)\r\n",
        "print('Input feature after playing', action)\r\n",
        "print(state.feature())\r\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Example 1 : \n",
            "   1 2 3\n",
            "A O X _\n",
            "B _ _ _\n",
            "C _ _ _\n",
            "record = A1 A2\n",
            "Input feature after playing A1 A2\n",
            "[[[1. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]]\n",
            "\n",
            " [[0. 1. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]]]\n",
            "Example 2 : \n",
            "   1 2 3\n",
            "A O X _\n",
            "B O _ X\n",
            "C _ _ _\n",
            "record = A1 A2 B1 A2 A1 B3\n",
            "Input feature after playing B1 A2 A1 B3\n",
            "[[[1. 0. 0.]\n",
            "  [1. 0. 0.]\n",
            "  [0. 0. 0.]]\n",
            "\n",
            " [[0. 1. 0.]\n",
            "  [0. 0. 1.]\n",
            "  [0. 0. 0.]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cAVei_494pQ"
      },
      "source": [
        "Small Convolutional neural network using PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MtrJHcd1LeB"
      },
      "source": [
        "# \n",
        "class Conv(nn.Module):\n",
        "    def __init__(self, filters0, filters1, kernel_size, bn=False):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(filters0, filters1, kernel_size, stride=1, padding=kernel_size//2, bias=False)\n",
        "        self.bn = None\n",
        "        if bn:\n",
        "            self.bn = nn.BatchNorm2d(filters1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.conv(x)\n",
        "        if self.bn is not None:\n",
        "            h = self.bn(h)\n",
        "        return h\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, filters):\n",
        "        super().__init__()\n",
        "        self.conv = Conv(filters, filters, 3, True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.relu(x + (self.conv(x)))\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yp7wSCbq1LeB"
      },
      "source": [
        "num_filters = 16\n",
        "num_blocks = 4\n",
        "\n",
        "class Representation(nn.Module):\n",
        "    ''' Conversion from observation to inner abstract state '''\n",
        "    def __init__(self, input_shape):\n",
        "        super().__init__()\n",
        "        self.input_shape = input_shape\n",
        "        self.board_size = self.input_shape[1] * self.input_shape[2]\n",
        "\n",
        "        self.layer0 = Conv(self.input_shape[0], num_filters, 3, bn=True)\n",
        "        self.blocks = nn.ModuleList([ResidualBlock(num_filters) for _ in range(num_blocks)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = F.relu(self.layer0(x))\n",
        "        for block in self.blocks:\n",
        "            h = block(h)\n",
        "        return h\n",
        "\n",
        "    def inference(self, x):\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            rp = self(torch.from_numpy(x).unsqueeze(0))\n",
        "        return rp.cpu().numpy()[0]\n",
        "\n",
        "class Prediction(nn.Module):\n",
        "    ''' Policy and value prediction from inner abstract state '''\n",
        "    def __init__(self, action_shape):\n",
        "        super().__init__()\n",
        "        self.board_size = np.prod(action_shape[1:])\n",
        "        self.action_size = action_shape[0] * self.board_size\n",
        "\n",
        "        self.conv_p1 = Conv(num_filters, 4, 1, bn=True)\n",
        "        self.conv_p2 = Conv(4, 1, 1)\n",
        "\n",
        "        self.conv_v = Conv(num_filters, 4, 1, bn=True)\n",
        "        self.fc_v = nn.Linear(self.board_size * 4, 1, bias=False)\n",
        "\n",
        "    def forward(self, rp):\n",
        "        h_p = F.relu(self.conv_p1(rp))\n",
        "        h_p = self.conv_p2(h_p).view(-1, self.action_size)\n",
        "\n",
        "        h_v = F.relu(self.conv_v(rp))\n",
        "        h_v = self.fc_v(h_v.view(-1, self.board_size * 4))\n",
        "\n",
        "        # range of value is -1 ~ 1\n",
        "        return F.softmax(h_p, dim=-1), torch.tanh(h_v)\n",
        "\n",
        "    def inference(self, rp):\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            p, v = self(torch.from_numpy(rp).unsqueeze(0))\n",
        "        return p.cpu().numpy()[0], v.cpu().numpy()[0][0]\n",
        "\n",
        "class Dynamics(nn.Module):\n",
        "    '''Abstract state transition'''\n",
        "    def __init__(self, rp_shape, act_shape):\n",
        "        super().__init__()\n",
        "        self.rp_shape = rp_shape\n",
        "        self.layer0 = Conv(rp_shape[0] + act_shape[0], num_filters, 3, bn=True)\n",
        "        self.blocks = nn.ModuleList([ResidualBlock(num_filters) for _ in range(num_blocks)])\n",
        "\n",
        "    def forward(self, rp, a):\n",
        "        h = torch.cat([rp, a], dim=1)\n",
        "        h = self.layer0(h)\n",
        "        for block in self.blocks:\n",
        "            h = block(h)\n",
        "        return h\n",
        "\n",
        "    def inference(self, rp, a):\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            rp = self(torch.from_numpy(rp).unsqueeze(0), torch.from_numpy(a).unsqueeze(0))\n",
        "        return rp.cpu().numpy()[0]\n",
        "\n",
        "class Net(nn.Module):\n",
        "    '''Whole net'''\n",
        "    def __init__(self,state):\n",
        "        super().__init__()\n",
        "        #state = State()\n",
        "        input_shape = state.feature().shape\n",
        "        action_shape = state.action_feature(0).shape\n",
        "        rp_shape = (num_filters, *input_shape[1:])\n",
        "\n",
        "        self.representation = Representation(input_shape)\n",
        "        self.prediction = Prediction(action_shape)\n",
        "        self.dynamics = Dynamics(rp_shape, action_shape)\n",
        "\n",
        "    def predict(self, state0, path):\n",
        "        '''Predict p and v from original state and path'''\n",
        "        outputs = []\n",
        "        x = state0.feature()\n",
        "        rp = self.representation.inference(x)\n",
        "        outputs.append(self.prediction.inference(rp))\n",
        "        for action in path:\n",
        "            a = state0.action_feature(action)\n",
        "            rp = self.dynamics.inference(rp, a)\n",
        "            outputs.append(self.prediction.inference(rp))\n",
        "        return outputs"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dfQrmCA1LeC",
        "outputId": "1883a1bb-eadc-4613-b254-873deb132db5"
      },
      "source": [
        "def show_net(net, state):\n",
        "    '''Display policy (p) and value (v)'''\n",
        "    print(state)\n",
        "    p, v = net.predict(state, [])[-1]\n",
        "    print('policy p = ')\n",
        "    print((p * 1000).astype(int).reshape((-1, *net.representation.input_shape[1:3])))\n",
        "    print('value v = ', v)\n",
        "    print()\n",
        "\n",
        "#  Outputs before training\n",
        "state = State().tictactoe()\n",
        "show_net(Net(state),state )"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   1 2 3\n",
            "A _ _ _\n",
            "B _ _ _\n",
            "C _ _ _\n",
            "record = \n",
            "policy p = \n",
            "[[[111 111 111]\n",
            "  [111 111 111]\n",
            "  [111 111 111]]]\n",
            "value v =  0.0\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woYqVrR7-BAW"
      },
      "source": [
        "Implementation of the MCTS (Monte Carlo Tree Search)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZo72FTa1LeD"
      },
      "source": [
        "class Tree:\n",
        "    def __init__(self, net):\n",
        "        self.net = net\n",
        "        self.nodes = {}\n",
        "\n",
        "    def search(self, state, path, rp, depth):\n",
        "        # Return the predicted value from the new state\n",
        "        key = state.record_string()\n",
        "        if len(path) > 0:\n",
        "            key += '|' + ' '.join(map(state.action2str, path))\n",
        "        if key not in self.nodes:\n",
        "            p, v = self.net.prediction.inference(rp)\n",
        "            self.nodes[key] = Node(p, v)\n",
        "            return v\n",
        "\n",
        "        # State transition by an action selected from bandit\n",
        "        node = self.nodes[key]\n",
        "        p = node.p\n",
        "        mask = np.zeros_like(p)\n",
        "        if depth == 0:\n",
        "            # Add noise to policy on the root node\n",
        "            p = 0.75 * p + 0.25 * np.random.dirichlet([0.15] * len(p))\n",
        "            # On the root node, we choose action only from legal actions\n",
        "            mask[state.legal_actions()] = 1\n",
        "            p *= mask\n",
        "            p /= p.sum() + 1e-16\n",
        "\n",
        "        n, q_sum = 1 + node.n, node.q_sum_all / node.n_all + node.q_sum\n",
        "        ucb = q_sum / n + 2.0 * np.sqrt(node.n_all) * p / n + mask * 4 # PUCB formula\n",
        "        best_action = np.argmax(ucb)\n",
        "\n",
        "        # Search next state by recursively calling this function\n",
        "        rp_next = self.net.dynamics.inference(rp, state.action_feature(best_action))\n",
        "        path.append(best_action)\n",
        "        q_new = -self.search(state, path, rp_next, depth + 1) # With the assumption of changing player by turn\n",
        "        node.update(best_action, q_new)\n",
        "\n",
        "        return q_new\n",
        "\n",
        "    def think(self, state, num_simulations, temperature = 0, show=False):\n",
        "        # End point of MCTS\n",
        "        if show:\n",
        "            print(state)\n",
        "        start, prev_time = time.time(), 0\n",
        "        for _ in range(num_simulations):\n",
        "            self.search(state, [], self.net.representation.inference(state.feature()), depth=0)\n",
        "\n",
        "            # Display search result on every second\n",
        "            if show:\n",
        "                tmp_time = time.time() - start\n",
        "                if int(tmp_time) > int(prev_time):\n",
        "                    prev_time = tmp_time\n",
        "                    root, pv = self.nodes[state.record_string()], self.pv(state)\n",
        "                    print('%.2f sec. best %s. q = %.4f. n = %d / %d. pv = %s'\n",
        "                          % (tmp_time, state.action2str(pv[0]), root.q_sum[pv[0]] / root.n[pv[0]],\n",
        "                             root.n[pv[0]], root.n_all, ' '.join([state.action2str(a) for a in pv])))\n",
        "\n",
        "        #  Return probability distribution weighted by the number of simulations\n",
        "        n = root = self.nodes[state.record_string()].n + 1\n",
        "        n = (n / np.max(n)) ** (1 / (temperature + 1e-8))\n",
        "        return n / n.sum()\n",
        "\n",
        "    def pv(self, state):\n",
        "        # Return principal variation (action sequence which is considered as the best)\n",
        "        s, pv_seq = copy.deepcopy(state), []\n",
        "        while True:\n",
        "            key = s.record_string()\n",
        "            if key not in self.nodes or self.nodes[key].n.sum() == 0:\n",
        "                break\n",
        "            best_action = sorted([(a, self.nodes[key].n[a]) for a in s.legal_actions()], key=lambda x: -x[1])[0][0]\n",
        "            pv_seq.append(best_action)\n",
        "            s.play(best_action)\n",
        "        return pv_seq\n",
        "\n",
        "class Node:\n",
        "    '''Search result of one abstract (or root) state'''\n",
        "    def __init__(self, p, v):\n",
        "        self.p, self.v = p, v\n",
        "        self.n, self.q_sum = np.zeros_like(p), np.zeros_like(p)\n",
        "        self.n_all, self.q_sum_all = 1, v / 2 # prior\n",
        "\n",
        "    def update(self, action, q_new):\n",
        "        # Update\n",
        "        self.n[action] += 1\n",
        "        self.q_sum[action] += q_new\n",
        "\n",
        "        # Update overall stats\n",
        "        self.n_all += 1\n",
        "        self.q_sum_all += q_new"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7x_JnFbR-E4D"
      },
      "source": [
        "Search with initialized net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_vz07zd1LeE",
        "outputId": "059270bb-c3cb-452e-cbc8-6ea719f85b17"
      },
      "source": [
        "state = State().tictactoe()\n",
        "\n",
        "tree = Tree(Net(state))\n",
        "tree.think(state, 100, show=True)\n",
        "\n",
        "tree = Tree(Net(state))\n",
        "tree.think(state.play('A1 C1 A2 C2'), 100, show=True)\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   1 2 3\n",
            "A _ _ _\n",
            "B _ _ _\n",
            "C _ _ _\n",
            "record = \n",
            "   1 2 3\n",
            "A O O _\n",
            "B _ _ _\n",
            "C X X _\n",
            "record = A1 C1 A2 C2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 0., 1.], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpVxBOV2-Hik"
      },
      "source": [
        "Training of neural net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PijUuwGh1LeF"
      },
      "source": [
        "batch_size = 32\n",
        "num_steps = 100\n",
        "\n",
        "def gen_target(ep, k):\n",
        "    '''Generate inputs and targets for training'''\n",
        "    # path, reward, observation, action, policy\n",
        "    turn_idx = np.random.randint(len(ep[0]))\n",
        "    ps, vs, ax = [], [], []\n",
        "    for t in range(turn_idx, turn_idx + k + 1):\n",
        "        if t < len(ep[0]):\n",
        "            p = ep[4][t]\n",
        "            a = ep[3][t]\n",
        "        else: # state after finishing game\n",
        "            # p is 0 (loss is 0)\n",
        "            p = np.zeros_like(ep[4][-1])\n",
        "            # random action selection\n",
        "            a = np.zeros(np.prod(ep[3][-1].shape), dtype=np.float32)\n",
        "            a[np.random.randint(len(a))] = 1\n",
        "            a = a.reshape(ep[3][-1].shape)\n",
        "        vs.append([ep[1] if t % 2 == 0 else -ep[1]])\n",
        "        ps.append(p)\n",
        "        ax.append(a)\n",
        "        \n",
        "    return ep[2][turn_idx], ax, ps, vs\n",
        "\n",
        "def train(episodes, net, opt):\n",
        "    '''Train neural net'''\n",
        "    p_loss_sum, v_loss_sum = 0, 0\n",
        "    p_loss_l = []\n",
        "    v_loss_l = []\n",
        "    net.train()\n",
        "    k = 4\n",
        "    for _ in range(num_steps):\n",
        "        x, ax, p_target, v_target = zip(*[gen_target(episodes[np.random.randint(len(episodes))], k) for j in range(batch_size)])\n",
        "        x = torch.from_numpy(np.array(x))\n",
        "        ax = torch.from_numpy(np.array(ax))\n",
        "        p_target = torch.from_numpy(np.array(p_target))\n",
        "        v_target = torch.FloatTensor(np.array(v_target))\n",
        "\n",
        "        # Change the order of axis as [time step, batch, ...]\n",
        "        ax = torch.transpose(ax, 0, 1)\n",
        "        p_target = torch.transpose(p_target, 0, 1)\n",
        "        v_target = torch.transpose(v_target, 0, 1)\n",
        "\n",
        "        p_loss, v_loss = 0, 0\n",
        "\n",
        "        # Compute losses for k (+ current) steps\n",
        "        for t in range(k + 1):\n",
        "            rp = net.representation(x) if t == 0 else net.dynamics(rp, ax[t - 1])\n",
        "            p, v = net.prediction(rp)\n",
        "            p_loss += torch.sum(-p_target[t] * torch.log(p))\n",
        "            v_loss += torch.sum((v_target[t] - v) ** 2)\n",
        "\n",
        "        p_loss_sum += p_loss.item()\n",
        "        v_loss_sum += v_loss.item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        (p_loss + v_loss).backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    num_train_datum = num_steps * batch_size\n",
        "    print('p_loss %f v_loss %f' % (p_loss_sum / num_train_datum, v_loss_sum / num_train_datum))\n",
        "    return net,p_loss_sum, v_loss_sum"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O75aD2Bf-LHS"
      },
      "source": [
        "# **Battle against random agents**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYXkTlt81LeF"
      },
      "source": [
        "def vs_random(net, n=100):\n",
        "    results = {}\n",
        "    for i in range(n):\n",
        "        first_turn = i % 2 == 0\n",
        "        turn = first_turn\n",
        "        state =  State().tictactoe()\n",
        "        while not state.terminal():\n",
        "            if turn:\n",
        "                p, _ = net.predict(state, [])[-1]\n",
        "                action = sorted([(a, p[a]) for a in state.legal_actions()], key=lambda x:-x[1])[0][0]\n",
        "            else:\n",
        "                action = np.random.choice(state.legal_actions())\n",
        "            state.play(action)\n",
        "            turn = not turn\n",
        "        r = state.terminal_reward() if turn else -state.terminal_reward()\n",
        "        results[r] = results.get(r, 0) + 1\n",
        "    return results"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsoC4XWT-OWL"
      },
      "source": [
        "Main algorithm of MuZero vs random agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwKvPfDQ1LeG",
        "outputId": "fe982fd4-10cc-4e19-8aa4-c0ecb1d74011"
      },
      "source": [
        "num_games = 100\n",
        "num_games_one_epoch = 50\n",
        "num_simulations = 50\n",
        "\n",
        "state_ttt = State().tictactoe()\n",
        "net = Net(state_ttt)\n",
        "optimizer = optim.SGD(net.parameters(), lr=3e-4, weight_decay=3e-5, momentum=0.8)\n",
        "\n",
        "# Display battle results as {-1: lose 0: draw 1: win} (for episode generated for training, 1 means that the first player won)\n",
        "vs_random_sum = vs_random(net)\n",
        "print('vs_random = ', sorted(vs_random_sum.items()))\n",
        "\n",
        "episodes = []\n",
        "result_distribution = {1: 0, 0: 0, -1: 0}\n",
        "plot_sum=[]\n",
        "plot_loss = []\n",
        "plot_value = []\n",
        "\n",
        "for g in range(num_games):\n",
        "    # Generate one episode\n",
        "    record, p_targets, features, action_features = [], [], [], []\n",
        "    #print(state)\n",
        "\n",
        "    state = State().tictactoe()\n",
        "    # temperature using to make policy targets from search results\n",
        "    temperature = 0.7\n",
        "    while not state.terminal():\n",
        "        tree = Tree(net)\n",
        "        p_target = tree.think(state, num_simulations, temperature)\n",
        "        p_targets.append(p_target)\n",
        "        features.append(state.feature())\n",
        "        # Select action with generated distribution, and then make a transition by that action\n",
        "        action = np.random.choice(np.arange(len(p_target)), p=p_target)\n",
        "        action_features.append(state.action_feature(action))\n",
        "        state.play(action)\n",
        "        record.append(action)\n",
        "        temperature *= 0.8\n",
        "    # reward seen from the first turn player\n",
        "    reward = state.terminal_reward() * (1 if len(record) % 2 == 0 else -1)\n",
        "    result_distribution[reward] += 1\n",
        "    episodes.append((record, reward, features, action_features, p_targets))\n",
        "    if g % num_games_one_epoch == 0:\n",
        "        print('game ', end='')\n",
        "    print(g, ' ', end='')\n",
        "\n",
        "    # Training of neural net\n",
        "    if (g + 1) % num_games_one_epoch == 0:\n",
        "        # Show the result distributiuon of generated episodes\n",
        "        #print('generated = ', sorted(result_distribution.items()))\n",
        "        net,loss,value = train(episodes, net, optimizer)\n",
        "        plot_loss.append(loss)\n",
        "        plot_value.append(value)\n",
        "\n",
        "        vs_random_once = vs_random(net)\n",
        "        #print('vs_random = ', sorted(vs_random_once.items()), end='')\n",
        "        for r, n in vs_random_once.items():\n",
        "            vs_random_sum[r] += n\n",
        "        #print(' sum = ', sorted(vs_random_sum.items()))\n",
        "        plot_sum.append(vs_random_once.items())\n",
        "\n",
        "        #show_net(net, State().tictactoe())\n",
        "        #show_net(net, State().play('A1 C1 A2 C2'))\n",
        "        #show_net(net, State().play('A1 B2 C3 B3 C1'))\n",
        "        #show_net(net, State().play('B2 A2 A3 C1 B3'))\n",
        "        #show_net(net, State().play('B2 A2 A3 C1'))\n",
        "print('finished')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vs_random =  [(-1, 36), (0, 15), (1, 49)]\n",
            "game 0  1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  p_loss 7.664570 v_loss 3.330757\n",
            "game 50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99  p_loss 6.909109 v_loss 3.578579\n",
            "finished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1Rok1yAAFik"
      },
      "source": [
        "Some fancy plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "ViTBp_6Cno7s",
        "outputId": "45151105-d44b-435f-9136-ef2e8d844f29"
      },
      "source": [
        "list_serie = [list(d) for d in plot_sum]\r\n",
        "win_serie = []\r\n",
        "lose_serie = []\r\n",
        "draw_serie = []\r\n",
        "\r\n",
        "for i in range(len(list_serie)):\r\n",
        "  sample = list_serie[i]\r\n",
        "  for j in range(len(sample)):\r\n",
        "    if sample[j][0]==1:\r\n",
        "      win_serie.append(sample[j][1])\r\n",
        "    elif sample[j][0]==-1:\r\n",
        "      lose_serie.append(sample[j][1])\r\n",
        "    else:\r\n",
        "      draw_serie.append(sample[j][1])\r\n",
        "\r\n",
        "plt.plot(win_serie,label='win')\r\n",
        "plt.plot(lose_serie,label='lose')\r\n",
        "plt.plot(draw_serie,label='draw')\r\n",
        "plt.legend()\r\n",
        "plt.xlabel('number of epochs')\r\n",
        "plt.ylabel('results')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'results')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "XVkuS-Wyzj6o",
        "outputId": "dbf8abbf-1cff-4fe1-d550-1b20fca948ea"
      },
      "source": [
        "plt.plot(plot_loss,label='policy prediction loss')\r\n",
        "plt.plot(plot_value,label='value prediction loss')\r\n",
        "plt.legend()\r\n",
        "plt.xlabel('number of epochs')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'number of epochs')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lbLYeCOEfzOL",
        "outputId": "c8af1b1a-4bd6-482b-b190-7395a6c6efcc"
      },
      "source": [
        "# Search with trained net\r\n",
        "tree = Tree(net)\r\n",
        "tree.think(State().tictactoe().play('A1 B3 C1 A3 '), 1000, show=True)\r\n",
        "#tree.pv(State().tictactoe().play('A1 C1 C3 C2'))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   1 2 3\n",
            "A O _ X\n",
            "B _ _ X\n",
            "C O _ _\n",
            "record = A1 B3 C1 A3\n",
            "1.00 sec. best A2. q = -0.6638. n = 88 / 326. pv = A2\n",
            "2.00 sec. best A2. q = -0.6822. n = 158 / 609. pv = A2\n",
            "3.00 sec. best A2. q = -0.6850. n = 227 / 867. pv = A2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 1., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8UgY6adf4FP",
        "outputId": "c81ea214-e9dc-4903-e45b-0835478cc8e4"
      },
      "source": [
        "print(vs_random(net, n=1000))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{1: 536, -1: 348, 0: 116}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhQ8zepEmjxW"
      },
      "source": [
        "**Playing agiant MuZero**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "id": "o6d25pM3mZ25",
        "outputId": "f48a238d-2af7-47e0-a63d-4a39048d579a"
      },
      "source": [
        "first_turn = True\r\n",
        "turn = first_turn\r\n",
        "state = State().tictactoe()\r\n",
        "while not state.terminal():\r\n",
        "    if turn:\r\n",
        "        p, _ = net.predict(state, [])[-1]\r\n",
        "        action = sorted([(a, p[a]) for a in state.legal_actions()], key=lambda x:-x[1])[0][0]\r\n",
        "    else:\r\n",
        "        action = input('Up to you ! Choose an action : ')\r\n",
        "        while not (action in [state.action2str(e) for e in state.legal_actions()] ):\r\n",
        "              print('Oups try again !available actions are : ',[state.action2str(e) for e in state.legal_actions()])\r\n",
        "              action = input( 'Choose an action  : ')\r\n",
        "\r\n",
        "    state.play(action)\r\n",
        "    turn = not turn\r\n",
        "    print(state)\r\n",
        "\r\n",
        "if state.terminal_reward() == -1:\r\n",
        "  print(\"Hey you won !!!\")\r\n",
        "elif state.terminal_reward() == 1:\r\n",
        "  print(\"Fuck he got me\")\r\n",
        "else:\r\n",
        "  print(\"That was close\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   1 2 3\n",
            "A _ _ _\n",
            "B _ _ O\n",
            "C _ _ _\n",
            "record = B3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    582\u001b[0m         \"\"\"\n\u001b[0;32m--> 583\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-b7fefadf4994>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegal_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Up to you ! Choose an action : '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction2str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegal_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m               \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Oups try again !available actions are : '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction2str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegal_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1pw9R4I-iv4"
      },
      "source": [
        "# **MuZero VS MuZero**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gM2D4ITfmqT2"
      },
      "source": [
        "# Training of neural net\r\n",
        "batch_size = 32\r\n",
        "num_steps = 100\r\n",
        "\r\n",
        "def gen_target(ep, k):\r\n",
        "    '''Generate inputs and targets for training'''\r\n",
        "    # path, reward, observation, action, policy\r\n",
        "    turn_idx = np.random.randint(len(ep[0]))\r\n",
        "    ps, vs, ax = [], [], []\r\n",
        "    for t in range(turn_idx, turn_idx + k + 1):\r\n",
        "        if t < len(ep[0]):\r\n",
        "            p = ep[4][t]\r\n",
        "            a = ep[3][t]\r\n",
        "        else: # state after finishing game\r\n",
        "            # p is 0 (loss is 0)\r\n",
        "            p = np.zeros_like(ep[4][-1])\r\n",
        "            # random action selection\r\n",
        "            a = np.zeros(np.prod(ep[3][-1].shape), dtype=np.float32)\r\n",
        "            a[np.random.randint(len(a))] = 1\r\n",
        "            a = a.reshape(ep[3][-1].shape)\r\n",
        "        vs.append([ep[1] if t % 2 == 0 else -ep[1]])\r\n",
        "        ps.append(p)\r\n",
        "        ax.append(a)\r\n",
        "        \r\n",
        "    return ep[2][turn_idx], ax, ps, vs\r\n",
        "\r\n",
        "def train(episodes, net, opt):\r\n",
        "    '''Train neural net'''\r\n",
        "    p_loss_sum, v_loss_sum = 0, 0\r\n",
        "    p_loss_l = []\r\n",
        "    v_loss_l = []\r\n",
        "    net.train()\r\n",
        "    k = 4\r\n",
        "    for _ in range(num_steps):\r\n",
        "        x, ax, p_target, v_target = zip(*[gen_target(episodes[np.random.randint(len(episodes))], k) for j in range(batch_size)])\r\n",
        "        x = torch.from_numpy(np.array(x))\r\n",
        "        ax = torch.from_numpy(np.array(ax))\r\n",
        "        p_target = torch.from_numpy(np.array(p_target))\r\n",
        "        v_target = torch.FloatTensor(np.array(v_target))\r\n",
        "\r\n",
        "        # Change the order of axis as [time step, batch, ...]\r\n",
        "        ax = torch.transpose(ax, 0, 1)\r\n",
        "        p_target = torch.transpose(p_target, 0, 1)\r\n",
        "        v_target = torch.transpose(v_target, 0, 1)\r\n",
        "\r\n",
        "        p_loss, v_loss = 0, 0\r\n",
        "\r\n",
        "        # Compute losses for k (+ current) steps\r\n",
        "        for t in range(k + 1):\r\n",
        "            rp = net.representation(x) if t == 0 else net.dynamics(rp, ax[t - 1])\r\n",
        "            p, v = net.prediction(rp)\r\n",
        "            p_loss += torch.sum(-p_target[t] * torch.log(p))\r\n",
        "            v_loss += torch.sum((v_target[t] - v) ** 2)\r\n",
        "\r\n",
        "        p_loss_sum += p_loss.item()\r\n",
        "        v_loss_sum += v_loss.item()\r\n",
        "\r\n",
        "        optimizer.zero_grad()\r\n",
        "        (p_loss + v_loss).backward()\r\n",
        "        optimizer.step()\r\n",
        "\r\n",
        "    num_train_datum = num_steps * batch_size\r\n",
        "    print('p_loss %f v_loss %f' % (p_loss_sum / num_train_datum, v_loss_sum / num_train_datum))\r\n",
        "    return net,p_loss_sum, v_loss_sum"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKH3yqQ2hQ7B"
      },
      "source": [
        "# Battle against muzero\r\n",
        "def vs_muzero(net, n=100):\r\n",
        "    results = {}\r\n",
        "    for i in range(n):\r\n",
        "        first_turn = i % 2 == 0\r\n",
        "        turn = first_turn\r\n",
        "        state = State().tictactoe()\r\n",
        "        while not state.terminal():\r\n",
        "            if turn:\r\n",
        "                p, _ = net.predict(state, [])[-1]\r\n",
        "                action = sorted([(a, p[a]) for a in state.legal_actions()], key=lambda x:-x[1])[0][0]\r\n",
        "            else:\r\n",
        "                p, _ = net.predict(state, [])[-1]\r\n",
        "                action = sorted([(a, p[a]) for a in state.legal_actions()], key=lambda x:-x[1])[0][0]\r\n",
        "            state.play(action)\r\n",
        "            turn = not turn\r\n",
        "        r = state.terminal_reward() if turn else -state.terminal_reward()\r\n",
        "        results[r] = results.get(r, 0) + 1\r\n",
        "    return results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPQsh1ON-MWs"
      },
      "source": [
        "# Main algorithm of MuZero vs MuZero\r\n",
        "\r\n",
        "num_games = 100\r\n",
        "num_games_one_epoch = 20\r\n",
        "num_simulations = 40\r\n",
        "\r\n",
        "state_ttt = State().tictactoe()\r\n",
        "net = Net(state_ttt)\r\n",
        "optimizer = optim.SGD(net.parameters(), lr=3e-4, weight_decay=3e-5, momentum=0.8)\r\n",
        "\r\n",
        "# Display battle results as {-1: lose 0: draw 1: win} (for episode generated for training, 1 means that the first player won)\r\n",
        "vs_muzero_sum = vs_muzero(net)\r\n",
        "print('vs_muzero = ', sorted(vs_muzero_sum.items()))\r\n",
        "\r\n",
        "episodes = []\r\n",
        "result_distribution = {1: 0, 0: 0, -1: 0}\r\n",
        "plot_sum=[]\r\n",
        "plot_loss = []\r\n",
        "plot_value = []\r\n",
        "\r\n",
        "for g in range(num_games):\r\n",
        "    # Generate one episode\r\n",
        "    record, p_targets, features, action_features = [], [], [], []\r\n",
        "    #print(state)\r\n",
        "\r\n",
        "    state = State().tictactoe()\r\n",
        "    # temperature using to make policy targets from search results\r\n",
        "    temperature = 0.7\r\n",
        "    while not state.terminal():\r\n",
        "        tree = Tree(net)\r\n",
        "        p_target = tree.think(state, num_simulations, temperature)\r\n",
        "        p_targets.append(p_target)\r\n",
        "        features.append(state.feature())\r\n",
        "        # Select action with generated distribution, and then make a transition by that action\r\n",
        "        p, _ = net.predict(state, [])[-1]\r\n",
        "        action = sorted([(a, p[a]) for a in state.legal_actions()], key=lambda x:-x[1])[0][0]\r\n",
        "        action_features.append(state.action_feature(action))\r\n",
        "        state.play(action)\r\n",
        "        record.append(action)\r\n",
        "        temperature *= 0.8\r\n",
        "    # reward seen from the first turn player\r\n",
        "    reward = state.terminal_reward() * (1 if len(record) % 2 == 0 else -1)\r\n",
        "    result_distribution[reward] += 1\r\n",
        "    episodes.append((record, reward, features, action_features, p_targets))\r\n",
        "    if g % num_games_one_epoch == 0:\r\n",
        "        print('game ', end='')\r\n",
        "    print(g, ' ', end='')\r\n",
        "\r\n",
        "    # Training of neural net\r\n",
        "    if (g + 1) % num_games_one_epoch == 0:\r\n",
        "        # Show the result distributiuon of generated episodes\r\n",
        "        #print('generated = ', sorted(result_distribution.items()))\r\n",
        "        net,loss,value = train(episodes, net, optimizer)\r\n",
        "        plot_loss.append(loss)\r\n",
        "        plot_value.append(value)\r\n",
        "\r\n",
        "        vs_muzero_once = vs_muzero(net)\r\n",
        "        #print('vs_muzero = ', sorted(vs_muzero_once.items()), end='')\r\n",
        "        #for r, n in vs_muzero_once.items():\r\n",
        "            #vs_muzero_sum[r] += n\r\n",
        "        #print(' sum = ', sorted(vs_muzero_sum.items()))\r\n",
        "        plot_sum.append(vs_muzero_once.items())\r\n",
        "\r\n",
        "        #show_net(net, State().tictactoe())\r\n",
        "        #show_net(net, State().play('A1 C1 A2 C2'))\r\n",
        "        #show_net(net, State().play('A1 B2 C3 B3 C1'))\r\n",
        "        #show_net(net, State().play('B2 A2 A3 C1 B3'))\r\n",
        "        #show_net(net, State().play('B2 A2 A3 C1'))\r\n",
        "print('finished')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tvaWJX9NqJc"
      },
      "source": [
        "list_serie = [list(d) for d in plot_sum]\r\n",
        "win_serie = []\r\n",
        "lose_serie = []\r\n",
        "draw_serie = []\r\n",
        "\r\n",
        "for i in range(len(list_serie)):\r\n",
        "  sample = list_serie[i]\r\n",
        "  for j in range(len(sample)):\r\n",
        "    if sample[j][0]==1:\r\n",
        "      win_serie.append(sample[j][1])\r\n",
        "    elif sample[j][0]==-1:\r\n",
        "      lose_serie.append(sample[j][1])\r\n",
        "    else:\r\n",
        "      draw_serie.append(sample[j][1])\r\n",
        "\r\n",
        "plt.plot(win_serie,label='win')\r\n",
        "plt.plot(lose_serie,label='lose')\r\n",
        "plt.plot(draw_serie,label='draw')\r\n",
        "plt.legend()\r\n",
        "plt.xlabel('number of epochs')\r\n",
        "plt.ylabel('Results')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBdQZk-PN7jh"
      },
      "source": [
        "plt.plot(plot_loss,label='policy prediction loss')\r\n",
        "plt.legend()\r\n",
        "plt.xlabel('number of epochs')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXL0Chmd1LeG"
      },
      "source": [
        "print(vs_muzero(net, n=1000))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NWzlHe21LeG"
      },
      "source": [
        "# Search with trained net\n",
        "tree = Tree(net)\n",
        "s = State().tictactoe().play('A1 A3 ')\n",
        "tree.think(s, 100, show=True)\n",
        "tree.pv(s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwnzOKmw7CTQ"
      },
      "source": [
        "# Show outputs from trained net\r\n",
        "print('initial state')\r\n",
        "show_net(net, State().tictactoe())\r\n",
        "\r\n",
        "print('WIN by put')\r\n",
        "show_net(net, State().tictactoe().play('A1 C1 A2 C2'))\r\n",
        "\r\n",
        "print('LOSE by opponent\\'s double')\r\n",
        "show_net(net, State().tictactoe().play('B2 A2 A3 C1 B3'))\r\n",
        "\r\n",
        "print('WIN through double')\r\n",
        "show_net(net, State().tictactoe().play('B2 A2 A3 C1'))\r\n",
        "\r\n",
        "# hard case: putting on A1 will cause double\r\n",
        "print('strategic WIN by following double')\r\n",
        "show_net(net, State().tictactoe().play('B1 A3'))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}